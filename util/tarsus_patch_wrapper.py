import torch
import torch.nn as nn


class Tarsus_Predictor_v2(nn.Module):
    def __init__(self, inp_dim, hidden_dim_ff, lstm_num_layers):
        super(Tarsus_Predictor_v2, self).__init__()

        self.fc_layers = nn.Sequential(
            nn.Linear(inp_dim, hidden_dim_ff),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim_ff, hidden_dim_ff),
            nn.Linear(hidden_dim_ff, hidden_dim_ff),
        )

        self.lstm = nn.LSTM(input_size=hidden_dim_ff,
                            hidden_size=hidden_dim_ff,
                            num_layers=lstm_num_layers, batch_first=False)

        self.out_layers = nn.Sequential(
            nn.Linear(hidden_dim_ff, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 64),
            nn.Linear(64, 2)
        )

    def reset_hidden_state(self, device=torch.device('cpu'), batch_size=None):
        self.hx = None

    def forward(self, inp):
        inp = self.fc_layers(inp)
        inp, self.hx = self.lstm(inp, self.hx)
        out = self.out_layers(inp)
        return out


class TarsusPatchWrapper():
    def __init__(self, actor):
        self.actor = actor
        self.encoder_patch = Tarsus_Predictor_v2(hidden_dim_ff=128, inp_dim=26, lstm_num_layers=2)
        self.encoder_patch.load_state_dict(torch.load("pretrained_models/tarsus_encoder/model-2023-08-22_23_49_43.118176-10010530_1.pth", map_location=torch.device('cpu')))
        self.encoder_patch.reset_hidden_state()
        self.encoder_patch.eval()
        self.encoder_input_idx = [7,8,9,10,11,
                                12,13,14,15,16,
                                17,18,19,20,21,
                                22,23,24,25,26,
                                27,29,30,
                                31,33,34]
        self.encoder_output_idx = [28, 32]

    def __getattr__(self, attr):
        return getattr(self.actor, attr)

    def __call__(self, *args, **kwargs):
        # manually defer this to .forward, as we cannot inherit from nn.Module
        return self.forward(*args, **kwargs)

    def forward(self, x, *args, **kwargs):
        x[self.encoder_output_idx] = self.encoder_patch.forward(x[self.encoder_input_idx].reshape(1,-1)).reshape(-1)
        return self.actor(x, *args, **kwargs)